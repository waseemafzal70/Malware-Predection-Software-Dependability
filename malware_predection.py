# -*- coding: utf-8 -*-
"""Malware Predection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16nFg-5q12yJjNKBP6kiR5Fpj_Kj3Y4E8

# **Part 1**
Combining all datasets into one master dataset
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

# Loading all the Malware Capture datasets into memory
set_1 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-42/detailed-bidirectional-flow-labels/capture20110810.binetflow')
set_2 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-43/detailed-bidirectional-flow-labels/capture20110811.binetflow')
set_3 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-44/detailed-bidirectional-flow-labels/capture20110812.binetflow')
set_4 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-45/detailed-bidirectional-flow-labels/capture20110815.binetflow')
set_5 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-46/detailed-bidirectional-flow-labels/capture20110815-2.binetflow')
set_6 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-47/detailed-bidirectional-flow-labels/capture20110816.binetflow')
set_7 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-48/detailed-bidirectional-flow-labels/capture20110816-2.binetflow')
set_8 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-49/detailed-bidirectional-flow-labels/capture20110816-3.binetflow')
set_9 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-50/detailed-bidirectional-flow-labels/capture20110817.binetflow')
set_10 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-51/detailed-bidirectional-flow-labels/capture20110818.binetflow')
set_11 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-52/detailed-bidirectional-flow-labels/capture20110818-2.binetflow')
set_12 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-53/detailed-bidirectional-flow-labels/capture20110819.binetflow')
set_13 = pd.read_csv('https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-54/detailed-bidirectional-flow-labels/capture20110815-3.binetflow')

# Combining all datasets into one master Malware Capture dataset
Complete_Dataset_df = pd.concat([set_1, set_2, set_3, set_4, set_5, set_6, set_7, set_8, set_9, set_10, set_11, set_12, set_13], ignore_index = True)

Complete_Dataset_df.tail()

# Labeling column to identify the malicious IP Addresses.
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.165', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.191', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.192', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.193', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.204', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.205', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.206', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.207', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.208', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.209', 'Malicious'] = 1
Complete_Dataset_df.loc[Complete_Dataset_df['Malicious'] != 1, 'Malicious'] = 0

# Printing the source IP addresses column to check if it's working properly.

display(Complete_Dataset_df.loc[Complete_Dataset_df['SrcAddr'] == '147.32.84.165'].head())
display(Complete_Dataset_df.head())

# one-hot encoding

Complete_Dataset_df.State.unique()

# This is very big dataset and I dont have that much high machine to work with the complete dataset. So, I'm dropping some columns that I am not using in this project.
Complete_Dataset_df.drop(['StartTime', 'SrcAddr', 'Sport', 'Dir', 'DstAddr', 'Dport', 'State', 'sTos', 'dTos', 'Label'], axis = 1, inplace = True)

Complete_Dataset_df

# I am using Colab free version so I dont have that much memory in free version to run the whole project at once so I'm dividing the project in parts.
# For now I'll save the dataset into my drive to use it in the next part of the project.

Complete_Dataset_df.to_csv( "/content/drive/MyDrive/Colab Notebooks/malware_dataset.csv", index=False, encoding='utf-8-sig')

"""# **Part 2**
Doing one hot encoding & saving thr updated dummy encoded dataframe to google drive
"""

import pandas as pd
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

# Reading the CSV file that we saved in part 1 to google drive
import pandas as pd
malware_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/malware_dataset.csv', error_bad_lines=False, index_col=False, dtype='unicode')

malware_dataset

# Time for one hot encoding of Proto. Usually I'd do this before making a permanent file, but just running the setup files eats most of a colab session worth of memory, so this will have to do

Proto_dummy = pd.get_dummies(malware_dataset.Proto)

# Combining the dummy frame with the original data frame
malware_dataset = pd.concat([malware_dataset, Proto_dummy], axis = 1)
malware_dataset

# Saving the updated dummy encoded dataframe 

with open('/content/drive/MyDrive/Colab Notebooks/updated_malware_dataset.csv', 'w') as f:
  malware_dataset.to_csv(f, header = True, index = False)

"""# **Part 3** 
Cleaning and balancing the final(cleaned) datafram & saving it to google drive for training and validating the ML model in part 4.
"""

import pandas as pd
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

# Reading the new dataframe
updated_malware_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/updated_malware_dataset.csv', error_bad_lines=False, index_col=False, dtype='unicode')

updated_malware_dataset.head()

# Setting the target

y = updated_malware_dataset.Malicious

# Counting how many malicious IP instances are in the dataset

target_counts = y.value_counts()
num_ones = target_counts[1]
print(num_ones)

# Extracting the malicious units

ones = updated_malware_dataset[updated_malware_dataset['Malicious'] == '1.0']

# Extracting the non-malicious records

zeroes = updated_malware_dataset[updated_malware_dataset['Malicious'] == '0.0']

# Taking a random sample of non-malicious units equal to the number of malicious records

zeroes_sample = zeroes.sample(n=len(ones))

# Ensuring that the data is balanced between malicious and non malicious data

print(f'Num zeroes: {len(zeroes_sample)}')
print(f'Num ones: {len(ones)}')

# Combining the malicious IP records with the non malicious sample

cleaned_dataset_df = pd.concat([ones, zeroes.sample(n=len(ones), replace=True)])

# Confirming proper concat

print(cleaned_dataset_df['Malicious'].value_counts())

# Saving the final dataframe as a CSV

with open('/content/drive/MyDrive/Colab Notebooks/cleaned_dataset_df.csv', 'w') as f:
  cleaned_dataset_df.to_csv(f, header = True, index = False)

"""# **Part 4**"""

# Reading the final csv to a dataframe
import pandas as pd
import warnings
warnings.filterwarnings('ignore')


from google.colab import drive
drive.mount('/content/drive')

cleaned_dataset_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/cleaned_dataset_df.csv', error_bad_lines=False, index_col=False, dtype='unicode')

cleaned_dataset_df.head ()

# Setting target

y = cleaned_dataset_df.Malicious

# Setting predictive features

#features = ['Dur', 'TotPkts', 'TotBytes', 'SrcBytes', 'arp', 'esp', 'gre', 'icmp', 'igmp', 'ipnip', 'ipv6', 'ipv6-icmp', 'ipx/spx', 'llc', 'pim', 'rarp', 'rsvp', 'rtcp', 'rtp', 'tcp', 'udp', 'udt', 'unas']
features = ['Dur', 'TotPkts', 'TotBytes', 'SrcBytes', 'arp', 'esp', 'icmp', 'igmp', 'ipv6', 'ipv6-icmp', 'ipx/spx', 'pim', 'rarp', 'rtcp', 'rtp', 'tcp', 'udp', 'udt', 'unas']

X = cleaned_dataset_df[features].copy()

pip install eli5

import eli5
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import roc_auc_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from eli5.sklearn import PermutationImportance
from sklearn.inspection import permutation_importance
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import plot_roc_curve
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
from sklearn import metrics
from matplotlib import pyplot as plt

# 80/20 train test split

X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, test_size = 0.2, random_state = 1)

# Defining an f1 testing function

def get_f1(max_leaf_nodes, X_train, X_test, y_train, y_test):
    model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    f1 = f1_score(y_test, predictions, pos_label = '1.0')
    return(f1)

# Finding the optimal leaf nodes for f1 score

for max_leaf_nodes in [5, 50, 500, 5000]:
    my_f1 = get_f1(max_leaf_nodes, X_train, X_test, y_train, y_test)
    print(max_leaf_nodes, my_f1)

# Defining a roc_auc testing function

def get_roc_auc(max_leaf_nodes, X_train, X_test, y_train, y_test):
    model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    roc_auc = roc_auc_score(y_test, predictions)
    return(roc_auc)

# Finding optimal max leaf nodes by roc_auc

for max_leaf_nodes in [5, 50, 500, 5000]:
    my_roc_auc = get_roc_auc(max_leaf_nodes, X_train, X_test, y_train, y_test)
    print(max_leaf_nodes, my_roc_auc)

#Increasing leaf nodes to 5000 would have marginal benefits for the model's f1 and roc_auc, but the runtime difference is not worth it for this specific toy example.

classifier_1 = RandomForestClassifier(max_leaf_nodes=500, random_state = 1)

classifier_1.fit(X_train, y_train)

# Generating predictions

predictions = classifier_1.predict(X_test)

# Accuracy score

accuracy_score(y_test, predictions)

# f1 score

f1_score(y_test, predictions, pos_label = '1.0')

# ROC AUC

roc_auc_score(y_test, predictions)

# Plotting the ROC curve for our viewing pleasure

y_pred_prob = classifier_1.predict_proba(X_test)[:,1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob, pos_label='1.0')
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()

# Generating confusion matrix to view true and false positive counts

labels = np.unique(y_test)
cm = confusion_matrix(y_test, predictions, labels=labels)
pd.DataFrame(cm, index=labels, columns=labels)

# Graphing feature importance
importances = classifier_1.feature_importances_
indices = np.argsort(importances)
plt.figure(figsize=(25,10))
plt.title('Feature Importances')
plt.bar(range(len(indices)), importances[indices], color='b', align='center')
plt.xticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

# Permutation importance shuffles each individual column's values, leaving all other features in place to see what sort of impact it has on the target. 
# In this instance, SrcBytes, Dur, TotBytes, UDP, etc... have the largest importance towards predicting malicious IPs, while something like ipv6 has less importance.
perm = PermutationImportance(classifier_1, random_state=1).fit(X_test, y_test)
eli5.show_weights(perm, feature_names = X_test.columns.tolist())

# SHAP explainer for a single row of data
row_to_show = 5
prediction_data = X_test.iloc[row_to_show]
prediction_data_array = prediction_data.values.reshape(1,-1)

classifier_1.predict_proba(prediction_data_array)

!pip install shap
import shap

explainer = shap.TreeExplainer(classifier_1)
shap_values = explainer.shap_values(prediction_data)

# SHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction that would be made if that feature took some baseline value.
# This visual shows that TotPkts, TotBytes, Dur, SrcBytes, and udp have the greatest impact on the predictions (this lines up very nicely with our permutation importance)
# TotPkts being equal to 2 and TotBytes being equal to 207 increase the prediction value, and the largest impacts come from Dur being 0.000167 and SrcBytes being 66.

shap.initjs() # This trusts javascript
shap.force_plot(explainer.expected_value[1], shap_values[1], prediction_data)

# This is an approximation of the previous plot using kernel explainer. Kernel SHAP is a method that uses a special weighted linear regression to compute the importance of each feature. Less accurate, but roughly the same story.

k_explainer = shap.KernelExplainer(classifier_1.predict_proba, shap.sample(X_train, 5))
k_shap_values = k_explainer.shap_values(prediction_data)
shap.initjs() # This trusts javascript
shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], prediction_data)

# SHAP Summary plot needs to run on a small sample of the testing data. (The X_test dataframe is far too large to run this against in a timely manner)

explainer = shap.TreeExplainer(classifier_1)
X_test_sample = X_test.sample(frac=0.05, replace=False, random_state=1)

X_test_sample.dtypes

# Coercing features to numeric to make sure shap.summary_plot colors properly

X_test_sample = X_test_sample.apply(pd.to_numeric, errors='coerce')

X_test_sample.dtypes

shap_values = explainer.shap_values(X_test_sample)
shap.summary_plot(shap_values[0], X_test_sample)

# Commented out IPython magic to ensure Python compatibility.
classifier_2 = GaussianNB()
pred = classifier_2.fit(X_train, y_train).predict(X_test)
print("Number of mislabeled points out of a total %d points : %d"
#   % (X_test.shape[0], (y_test != pred).sum()))

accuracy_score(y_test, pred)

f1_score(y_test, pred, pos_label = '1.0')

pred = pred.astype(np.float64)
roc_auc_score(y_test, pred)

y_test = y_test.astype(np.float64)
labels = np.unique(y_test)
cm = confusion_matrix(y_test, pred, labels=labels)
pd.DataFrame(cm, index=labels, columns=labels)